{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frontiere_new(f, X, y, w = None, step = 50, alpha_choice = 1, aspect = 'auto', colorbar = True,\n",
    "                  samples = True):\n",
    "    \"\"\" trace la frontiere pour la fonction de decision f\"\"\"\n",
    "    # construct cmap\n",
    "\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    xx, yy = np.meshgrid(np.arange(min_tot0, max_tot0, delta0 / step),\n",
    "                         np.arange(min_tot1, max_tot1, delta1 / step))\n",
    "    z = np.array([f([vec]) for vec in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    z = z.reshape(xx.shape)\n",
    "    labels = np.unique(z)\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", labels.shape[0])\n",
    "    sns.set_palette(color_blind_list)\n",
    "    my_cmap = ListedColormap(color_blind_list)\n",
    "    plt.imshow(z, origin = 'lower', interpolation = \"mitchell\", alpha = 0.80,\n",
    "               cmap = my_cmap, extent = [min_tot0, max_tot0, min_tot1, max_tot1], aspect = 'auto')\n",
    "    if colorbar is True:\n",
    "        ax = plt.gca()\n",
    "        cbar = plt.colorbar(ticks = labels)\n",
    "        cbar.ax.set_yticklabels(labels)\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    k = np.unique(y).shape[0]\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", k)\n",
    "    sns.set_palette(color_blind_list)\n",
    "    ax = plt.gca()\n",
    "    if samples is True:\n",
    "        for i, label in enumerate(y):\n",
    "            label_num = np.where(labels == label)[0][0]\n",
    "            plt.scatter(X[i, 0], X[i, 1], c = color_blind_list[label_num],\n",
    "                       s=80, marker = symlist[label_num])\n",
    "            #plt.scatter(X[i, 0], X[i, 1], s=80)\n",
    "    plt.xlim([min_tot0, max_tot0])\n",
    "    plt.ylim([min_tot1, max_tot1])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    if w is not None:\n",
    "        plt.plot([min_tot0, max_tot0],\n",
    "                 [min_tot0 * -w[1] / w[2] - w[0] / w[2],\n",
    "                  max_tot0 * -w[1] / w[2] - w[0] / w[2]],\n",
    "                 \"k\", alpha=alpha_choice)\n",
    "\n",
    "        \n",
    "###############################################################################\n",
    "#               Algorithms and functions\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "class ErrorCurve(object):\n",
    "    def __init__(self, k_range = None, weights = 'uniform'):\n",
    "        if k_range is None:\n",
    "            k_range = list(range(1, 6))\n",
    "        self.k_range = k_range\n",
    "        self.weights = weights\n",
    "\n",
    "    def fit_curve(self, X, y, Xtest, ytest):\n",
    "        def error_func(k):\n",
    "            knn = neighbors.KNeighborsClassifier(n_neighbors=k,\n",
    "                                                 weights=self.weights)\n",
    "            knn.fit(X, y)\n",
    "            error = np.mean(knn.predict(Xtest) != ytest)\n",
    "            return error\n",
    "\n",
    "        errors = list(map(error_func, self.k_range))\n",
    "        self.errors = np.array(errors)\n",
    "        self.y = y\n",
    "\n",
    "    def plot(self, marker = 'o',color = 'b',label = None, maketitle = True, **kwargs):\n",
    "        plt.plot(self.k_range, self.errors, marker = marker, color = color,label = label,**kwargs)\n",
    "        plt.xlabel(\"K\")\n",
    "        plt.ylabel(\"Test error\")\n",
    "        if maketitle:\n",
    "            plt.title(\"number of training points : %d\" % len(self.y))\n",
    "Import libraries\n",
    "In [2]:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "import pandas as pd\n",
    "import tp_knn_source\n",
    "import scipy.spatial.distance\n",
    "import statsmodels\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from tp_knn_source import rand_bi_gauss,rand_tri_gauss,rand_checkers,rand_clown,rand_gauss,plot_2d, frontiere, frontiere_new, LOOCurve\n",
    "from tp_knn_source import ErrorCurve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets, metrics\n",
    "import statsmodels\n",
    "from scipy.stats import mode\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "/home/karine/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
    "  return f(*args, **kwds)\n",
    "Génération artificielle des données\n",
    "Question 1 : Etude des différentes distributions\n",
    "In [3]:\n",
    "#Données #1 \n",
    "#paramètres égaux à ceux du TP1\n",
    "#Je mets n1 et n2 un peu plus grand que ce qui est demandé pour que l'on puisse mieux voir les classifications\n",
    "#dans la visualisation à la question 5\n",
    "X1, y1 = rand_bi_gauss(n1 = 30, n2 = 30, mu1 = [1, 1], mu2 = [-1, -1],\n",
    "                      sigmas1 = [0.9, 0.9], sigmas2 = [0.9, 0.9])\n",
    "\n",
    "#Données #2\n",
    "X2, y2 = rand_tri_gauss(n1 = 50, n2 = 50, n3 = 50, mu1 = [1, 1], mu2 = [-1, -1], mu3 = [1, -1], \n",
    "                       sigma1 = [0.9, 0.9], sigma2 = [0.9, 0.9], sigma3 = [0.9, 0.9])\n",
    "\n",
    "#Données #3 \n",
    "#paramètres égaux à ceux du TP1\n",
    "n1 = 50\n",
    "n2 = 50\n",
    "sigmas1 = 1. \n",
    "sigmas2 = 5.\n",
    "X3, y3 = rand_clown(n1 , n2, sigmas1, sigmas2)\n",
    "\n",
    "#Données #4\n",
    "n1 = 150\n",
    "n2 = 150\n",
    "sigma = 0.9\n",
    "X4, y4 = rand_checkers(n1, n2, sigma)\n",
    "Description des fonctions\n",
    "La fonction <font color=blue>rand_bi_gauss <font color=black>génére un mélange aléatoire de 2 distributions gaussienne de paramètre respectives (mu1, sigma1) et (mu2, sigma2), étant le couple (moyenne, variance) pour chaque distribution. Le nombre de points généré pour chaque distribution est n1 pour la première distribution et n2 pour la 2eme distribution.\n",
    "La fonction <font color=blue>rand_tri_gauss <font color=black>génére un mélange aléatoire de 3 distributions gaussienne (similaire à la fonction bi_gauss).\n",
    "La fonction <font color=blue>rand_clown<font color=black> génére un mélange aléatoire de 2 distribution: une distribution parabolique de variance sigma et une distribution de moyenne nulle.\n",
    "La fonction <font color=blue>rand_checkers<font color=black> génére un damier aléatoire.\n",
    "Que représente la dernière colonne (y) ?\n",
    "Elle représente la classe (appelé aussi label) associée à chaque point x.\n",
    "\n",
    "Question 2 : Représentation graphique des distributions\n",
    "In [4]:\n",
    "fig=plt.figure(1, figsize=(15, 10))\n",
    "\n",
    "plt.subplot(221)\n",
    "plot_2d(X1, y1)\n",
    "plt.title('First data set')\n",
    "\n",
    "plt.subplot(222)\n",
    "plot_2d(X2, y2)\n",
    "plt.title('Second data set')\n",
    "\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.subplot(223)\n",
    "plot_2d(X3, y3)\n",
    "plt.title('Third data set')\n",
    "\n",
    "\n",
    "plt.subplot(224)\n",
    "plot_2d(X4, y4)\n",
    "plt.title('Fourth data set')\n",
    "Out[4]:\n",
    "Text(0.5,1,'Fourth data set')\n",
    "\n",
    "Observations\n",
    "Nous pouvons observés pour les données 1 et 2 des données provenant d'un mélange de lois gaussienne. On peut en déduire qu'elles sont séparables linéairement. Appliquer un LDA à ce jeu de donnée est pertinent. En revanche pour les données 3 provenant d'un mélange de lois non gaussienne, on peut se rendre compte qu'elles ne peuvent pas être classifiable par la méthode LDA. Cependant les classer par un classifieur k-nn est pertinent vu la distribution des données\n",
    "\n",
    "k-plus proches voisins\n",
    "Question 3 : k-plus proches voisins regression\n",
    "Pour le cas de la régression, quand les observations y sont à valeurs réelles, on associe à chaque groupe de k-voisins la moyenne de ces points.\n",
    "\n",
    "Question 4 : Classe k-nn classification artisanale\n",
    "In [5]:\n",
    "from scipy.stats import mode\n",
    "class KNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "#\"\"\" Homemade kNN classifier class \"\"\"\n",
    "    #Define the training set\n",
    "    def __init__(self, n_neighbors=1):\n",
    "        self.n_neighbors = n_neighbors\n",
    "    # Complete the mehtod\n",
    "    def fit(self, X, y):\n",
    "        self.xtrain = X\n",
    "        self.ytrain = y\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # Une ligne correspond à un point du tes set et chaque colonne\n",
    "        # à un point du training set\n",
    "        Md = cdist(X_test, self.xtrain, \"euclidean\")\n",
    "        # Ordonner la liste des voisins training par points du test set\n",
    "        neighbors_all = np.argsort(Md, axis =1)\n",
    "        neigh = neighbors_all[:,:self.n_neighbors]\n",
    "        Y = self.ytrain[neigh]\n",
    "        majority, nb = mode(Y, axis = 1) \n",
    "        Y_pred = majority.squeeze()\n",
    "        return Y_pred\n",
    "In [6]:\n",
    "test = KNNClassifier(3)\n",
    "test.fit(X2[::2],y2[::2])\n",
    "artisanal_label = test.predict(X2[1::2])\n",
    "print(f\"Les labels prédits pour la fonction test avec un modèle knn voisins ayant 3 voisins : \")\n",
    "print(artisanal_label)\n",
    "Les labels prédits pour la fonction test avec un modèle knn voisins ayant 3 voisins : \n",
    "[3. 1. 2. 3. 3. 2. 3. 1. 3. 2. 2. 2. 1. 1. 3. 3. 3. 1. 2. 1. 2. 2. 2. 1.\n",
    " 1. 2. 1. 2. 3. 1. 2. 3. 3. 3. 3. 2. 1. 1. 2. 1. 1. 3. 2. 2. 1. 2. 3. 3.\n",
    " 3. 1. 3. 3. 1. 2. 1. 1. 3. 1. 1. 1. 3. 2. 1. 3. 1. 1. 2. 3. 1. 3. 3. 1.\n",
    " 3. 1. 2.]\n",
    "Comparaison avec sklearn\n",
    "Résultats avec sklearn\n",
    "In [7]:\n",
    "from sklearn import neighbors, datasets\n",
    "clf = neighbors.KNeighborsClassifier(3)\n",
    "clf.fit(X2[::2],y2[::2])\n",
    "label_predict_sklearn = clf.predict(X2[1::2])\n",
    "print(f\"Les labels prédits pour la fonction test avec un modèle knn voisins ayant 3 voisins : \")\n",
    "print(label_predict_sklearn)\n",
    "Les labels prédits pour la fonction test avec un modèle knn voisins ayant 3 voisins : \n",
    "[3. 1. 2. 3. 3. 2. 3. 1. 3. 2. 2. 2. 1. 1. 3. 3. 3. 1. 2. 1. 2. 2. 2. 1.\n",
    " 1. 2. 1. 2. 3. 1. 2. 3. 3. 3. 3. 2. 1. 1. 2. 1. 1. 3. 2. 2. 1. 2. 3. 3.\n",
    " 3. 1. 3. 3. 1. 2. 1. 1. 3. 1. 1. 1. 3. 2. 1. 3. 1. 1. 2. 3. 1. 3. 3. 1.\n",
    " 3. 1. 2.]\n",
    "Comparaison entre la library sklearn et la library artisanale\n",
    "Comparaison labels prédits des 2 librairies\n",
    "In [8]:\n",
    "label_predict_sklearn == artisanal_label\n",
    "Out[8]:\n",
    "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
    "        True,  True,  True])\n",
    "Frontière de prédiction pour comparer les 2 librairies : sklearn et fait-maison\n",
    "In [9]:\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Avec librairie fait-maison\")\n",
    "def f_artis(x):\n",
    "    return test.predict(x)\n",
    "\n",
    "frontiere_new(f_artis, X2[1::2], y2[1::2])\n",
    "plt.subplot(122)\n",
    "plt.title(\"Avec librairie sklearn\")\n",
    "def f_sklearn(x):\n",
    "    return clf.predict(x)\n",
    "frontiere_new(f_sklearn,X2[1::2],y2[1::2])\n",
    "\n",
    "La librarie artisanale de knn voisins donne les mêmes résultats que la librairie sklearn.\n",
    "\n",
    "Question 5 : Visualisation de la frontière du k-nn classifieur sur les jeux de données\n",
    "In [10]:\n",
    "n_voisins = 5\n",
    "clf = neighbors.KNeighborsClassifier(n_voisins)\n",
    "In [11]:\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(221)\n",
    "\n",
    "clf.fit(X1[::2],y1[::2])\n",
    "def f(x):\n",
    "        return clf.predict(x)\n",
    "clf.predict(X1[1::2])\n",
    "\n",
    "plt.title('First data set')\n",
    "frontiere_new(f, X1[1::2], y1[1::2])\n",
    "#frontiere()\n",
    "plt.subplot(222)\n",
    "\n",
    "clf.fit(X2[::2],y2[::2])\n",
    "def f(x):\n",
    "        return clf.predict(x)\n",
    "\n",
    "\n",
    "plt.title('Second data set')\n",
    "frontiere_new(f, X2[1::2], y2[1::2])\n",
    "plt.subplot(223)\n",
    "plt.title('Third data set')\n",
    "\n",
    "clf.fit(X3[::2],y3[::2])\n",
    "def f(x):\n",
    "        return clf.predict(x)\n",
    "clf.predict(X3[1::2])\n",
    "frontiere_new(f,X3[1::2], y3[1::2])\n",
    "\n",
    "clf.fit(X4[::2],y4[::2])\n",
    "def f(x):\n",
    "        return clf.predict(x)\n",
    "clf.predict(X4[1::2])\n",
    "plt.subplot(224)\n",
    "plt.title('Fourth data set')\n",
    "frontiere_new(f, X4[1::2], y4[1::2])\n",
    "\n",
    "Observation\n",
    "Nos jeux de données sont relativement bien classées avec quelques erreurs. Plus nos données sont générées par des distributions complexes (non gaussienne), moins la classification est bonne.\n",
    "\n",
    "Question 6 : Influence du nombre de voisins sur les frontières de décision\n",
    "In [12]:\n",
    "plt.figure(3, figsize=(12, 8))\n",
    "iterate=0\n",
    "for n_neighbors in range(1, 60, 8):\n",
    "    iterate=iterate+1\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X2[::2],y2[::2])\n",
    "    plt.subplot(4, 2, iterate)\n",
    "    plt.xlabel('KNN with k=%d' % n_neighbors)\n",
    "    def f(x):\n",
    "        return knn.predict(x)\n",
    "    frontiere_new(f, X2[1::2], y2[1::2])\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "Observation :\n",
    "Les frontières se simplifient avec k qui grandit.\n",
    "\n",
    "Faisons un zoom sur pour 2 valeurs de k extrèmes.\n",
    "\n",
    "2 Valeurs de k extrèmes\n",
    "In [13]:\n",
    "plt.figure(3, figsize=(15, 8))\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X2[::2],y2[::2])\n",
    "def f(x):\n",
    "        return knn.predict(x)\n",
    "plt.subplot(121)\n",
    "plt.title('1 neighbor')\n",
    "frontiere_new(f, X2[1::2], y2[1::2])\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=50)\n",
    "knn.fit(X2[::2],y2[::2])\n",
    "def f(x):\n",
    "        return knn.predict(x)\n",
    "plt.subplot(122)\n",
    "plt.title('50 neighbors')\n",
    "frontiere_new(f, X2[1::2], y2[1::2])\n",
    "\n",
    "Observations :\n",
    "On remarque que plus le nombre de voisins, et plus les frontières entre les classes sont lissées. En effet, en augmentant le nombre de voisins, on lisse les différences et les frontières deviennent simples.\n",
    "\n",
    "Quand k = 1, la frontière est très complexe.\n",
    "Quand k = n, la frontière est simple.\n",
    "Question 7 : Pondérer les poids du jème voisin\n",
    "Le poids attribuée au j-eme voisin est$$ w_{j}=\\exp{(-\\frac{d_{j}^{2}}{h})} $$\n",
    "\n",
    "In [14]:\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "def weights(dist):\n",
    "            h_ = 0.01\n",
    "            w = np.exp(-dist**2/h_)\n",
    "            return w\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 7, weights = weights )\n",
    "knn.fit(X2[::2],y2[::2]) \n",
    "def f(x):\n",
    "    return knn.predict(x)\n",
    "plt.subplot(3,2,1)  \n",
    "plt.xlabel('KNN with h = 0.01')\n",
    "frontiere_new(f,X2[1::2],y2[1::2])\n",
    "\n",
    "def weights(dist):\n",
    "            h_ = 0.1\n",
    "            w = np.exp(-dist**2/100)\n",
    "            return w\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 7, weights = weights )\n",
    "knn.fit(X2[::2],y2[::2]) \n",
    "def f(x):\n",
    "    return knn.predict(x)\n",
    "plt.subplot(3,2,2)  \n",
    "plt.xlabel('KNN with h = 0.1')\n",
    "frontiere_new(f,X2[1::2],y2[1::2])\n",
    "\n",
    "def weights(dist):\n",
    "            h_ = 1\n",
    "            w = np.exp(-dist**2)\n",
    "            return w\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 7, weights = weights )\n",
    "knn.fit(X2[::2],y2[::2]) \n",
    "def f(x):\n",
    "    return knn.predict(x)\n",
    "plt.subplot(3,2,3)  \n",
    "plt.xlabel('KNN with h = 1')\n",
    "frontiere_new(f,X2[1::2],y2[1::2])\n",
    "\n",
    "def weights(dist):\n",
    "            h_ = 10\n",
    "            w = np.exp(-dist**2/h_)\n",
    "            return w\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 7, weights = weights )\n",
    "knn.fit(X2[::2],y2[::2]) \n",
    "def f(x):\n",
    "    return knn.predict(x)\n",
    "plt.subplot(3,2,4)  \n",
    "plt.xlabel('KNN with h = 10')\n",
    "frontiere_new(f,X2[1::2],y2[1::2])\n",
    "\n",
    "\n",
    "def weights(dist):\n",
    "            h_ = 100\n",
    "            w = np.exp(-dist**2/h_)\n",
    "            return w\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 7, weights = weights )\n",
    "knn.fit(X2[::2],y2[::2]) \n",
    "def f(x):\n",
    "    return knn.predict(x)\n",
    "plt.subplot(3,2,5)  \n",
    "plt.xlabel('KNN with h = 100')\n",
    "frontiere_new(f,X2[1::2],y2[1::2])\n",
    "\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 7, weights = None )\n",
    "knn.fit(X2[::2],y2[::2]) \n",
    "def f(x):\n",
    "    return knn.predict(x)\n",
    "plt.subplot(3,2,6)  \n",
    "plt.xlabel('Sans pondération')\n",
    "frontiere_new(f,X2[1::2],y2[1::2])\n",
    "\n",
    "Observation\n",
    "Il est difficile de juger de l'amélioration des résultats avec la pondération. On remarque une légère différence dans le lissage de la frontière avec h plus grand.\n",
    "\n",
    "Question 8: Taux d'erreur\n",
    "Taux d'erreur sur les données d'apprentissage\n",
    "In [15]:\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(X2[::2],y2[::2])\n",
    "erreur=1-knn.score(X2[::2],y2[::2])\n",
    "print(\"Le taux d'erreur sur les données test est : \" + str(round(erreur,3)))\n",
    "Le taux d'erreur sur les données test est : 0.0\n",
    "Taux d'erreur sur les données de test\n",
    "In [16]:\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(X2[::2],y2[::2])\n",
    "erreur=1-knn.score(X2[1::2],y2[1::2])\n",
    "print(\"Le taux d'erreur sur les données test est : \" + str(round(erreur,3)))\n",
    "Le taux d'erreur sur les données test est : 0.227\n",
    "Observation : \"Overfitting\"\n",
    "On remarque ainsi que notre modèle \"overfit\", surraprend les données d'entrainement. Il faut donc enlever un peu de complexité au modèle pour éviter cet \"overfitting\".\n",
    "\n",
    "Question 9 : Tracer le taux d'erreur en fonction de k\n",
    "In [17]:\n",
    "#Données #4\n",
    "n1 = 150\n",
    "n2 = 150\n",
    "sigma = 0.9\n",
    "X4, y4 = rand_checkers(n1, n2, sigma)\n",
    "X4_error = ErrorCurve(k_range=list(range(1,50)))\n",
    "X4_error.fit_curve(X4[::2],y4[::2], X4[1::2],y4[1::2])\n",
    "X4_error.plot(maketitle=False,marker='+',color='blue')\n",
    "plt.legend()\n",
    "No handles with labels found to put in legend.\n",
    "Out[17]:\n",
    "<matplotlib.legend.Legend at 0x7f33597685c0>\n",
    "\n",
    "Pour ce jeu de données, le meilleur k (celui qui donne l'erreur la plus faible) est 30. On évite de prendre un trop grand k par peur de overfitting.\n",
    "\n",
    "Question 10 : Tracer le taux d'erreur en fonction des échantillons et du k\n",
    "In [18]:\n",
    "samples=[100,200,500,1000]\n",
    "m=['+','o','*','v','h']\n",
    "c=['b','r','g','m','c']\n",
    "sigma = 0.1\n",
    "plt.figure(4, figsize=(15, 8))\n",
    "for iteration, sample in enumerate(samples):\n",
    "    label='Echantillon ' +str(sample)\n",
    "    n1 = sample\n",
    "    n2 = sample\n",
    "    X_train, Y_train = rand_checkers(n1, n2, sigma)\n",
    "    X_test, Y_test = rand_checkers(n1, n2, sigma)\n",
    "    X4_error = ErrorCurve(k_range=list(range(1,50)))\n",
    "    X4_error.fit_curve(X_train,Y_train, X_test,Y_test)\n",
    "    marker_ = m[iteration]\n",
    "    color_ = c[iteration]\n",
    "    \n",
    "    X4_error.plot(maketitle=False,marker=marker_,color=color_,label=label)\n",
    "    plt.legend()\n",
    "\n",
    "Influence de la taille de l'échantillon sur le taux d'erreur\n",
    "On observe que plus l'échantillon est grand et moins l'erreur est grande.\n",
    "De plus on remarque que le meilleur k n'est pas le même pour les différentes taille d'échantillon. Ce qui est logique.\n",
    "Meilleure k (celui qui donne un taux d'erreur le plus faible)\n",
    "Pour l'échantillon de taille 100, le meilleur k est 1\n",
    "Pour l'échantillon de taille 200, le meilleur k est 13\n",
    "Pour l'échantillon de taille 500, le meilleur k est 20\n",
    "Pour l'échantillon de taille 1000, le meilleur k est 20\n",
    "Visualisation des donnéers et la régle de décision pour échantillon = 1000\n",
    "In [19]:\n",
    "n1 = n2 = 1000\n",
    "sigma = 0.9\n",
    "X_train, Y_train = rand_checkers(n1, n2, sigma)\n",
    "X_test, Y_test = rand_checkers(n1, n2, sigma)\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors = 10)\n",
    "knn.fit(X_train, Y_train)\n",
    "def f(x):\n",
    "        return knn.predict(x)\n",
    "frontiere_new(f, X_test, Y_test)\n",
    "\n",
    "Question 11 : Avantages et inconvénients de la méthode des plus proches voisins\n",
    "Avantage\n",
    "La méthode des plus proches voisins est interprétable car le concept est simple.\n",
    "\n",
    "Inconvénients\n",
    "Le temps de calcul est plus important que d'autres classifieurs car il faut à chaque fois, mesurer la distance entre tous les points. Elle est donc très gourmande en temps et mémoire. Il est donc très coûteux de l'utiliser pour des problèmes à grande dimension. On peut donc avoir des problèmes importants de passage à l'échelle.\n",
    "\n",
    "Question 12 : Base Digits de scikit-lean\n",
    "Nature et format des données\n",
    "In [20]:\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "Les données sont des images de chiffres pixellisées.\n",
    "\n",
    "Histogramme pour des classes\n",
    "In [21]:\n",
    "\n",
    "Out[21]:\n",
    "Text(0,0.5,'Density')\n",
    "\n",
    "Observations\n",
    "Les classes sont réparties de façon équitable\n",
    "\n",
    "Knn classification appliquée à la base des digits\n",
    "In [22]:\n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: a support vector classifier\n",
    "\n",
    "classifier=neighbors.KNeighborsClassifier(n_neighbors=30)\n",
    "# We learn the digits on the first half of the digits\n",
    "\n",
    "classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n",
    "# Now predict the value of the digit on the second half:\n",
    "\n",
    "expected= digits.target[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])\n",
    "\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(f\"        Le taux d'erreur est {round(1-metrics.r2_score(expected, predicted),3)}\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "print(\"Les différentes métrique du classifieur %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"---------------------------------------------------------------------------\")\n",
    "---------------------------------------------------------------------------\n",
    "        Le taux d'erreur est 0.172\n",
    "---------------------------------------------------------------------------\n",
    "---------------------------------------------------------------------------\n",
    "---------------------------------------------------------------------------\n",
    "Les différentes métrique du classifieur KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=30, p=2,\n",
    "           weights='uniform'):\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.96      0.99      0.97        88\n",
    "          1       0.93      0.90      0.92        91\n",
    "          2       0.92      0.92      0.92        86\n",
    "          3       0.83      0.89      0.86        91\n",
    "          4       0.99      0.95      0.97        92\n",
    "          5       0.89      0.93      0.91        91\n",
    "          6       0.96      1.00      0.98        91\n",
    "          7       0.93      0.99      0.96        89\n",
    "          8       0.96      0.77      0.86        88\n",
    "          9       0.91      0.91      0.91        92\n",
    "\n",
    "avg / total       0.93      0.93      0.92       899\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "Question 13: Matrice de confusion associée au classifieur\n",
    "In [23]:\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))\n",
    "Confusion matrix:\n",
    "[[87  0  0  0  1  0  0  0  0  0]\n",
    " [ 0 82  5  1  0  1  0  0  0  2]\n",
    " [ 1  0 79  6  0  0  0  0  0  0]\n",
    " [ 0  0  0 81  0  3  0  4  1  2]\n",
    " [ 2  0  0  0 87  0  0  2  1  0]\n",
    " [ 0  0  0  0  0 85  3  0  0  3]\n",
    " [ 0  0  0  0  0  0 91  0  0  0]\n",
    " [ 0  0  1  0  0  0  0 88  0  0]\n",
    " [ 0  6  1  6  0  4  1  1 68  1]\n",
    " [ 1  0  0  4  0  2  0  0  1 84]]\n",
    "Question 14: Choix de k grâce à la validation croisée\n",
    "In [24]:\n",
    "rangek = [1,6,11,16,21,26,31,36,41,46,100,200,300]\n",
    "\n",
    "CV_digits = LOOCurve(k_range = rangek)\n",
    "CV_digits.fit_curve(data, digits.target)\n",
    "CV_digits.plot()\n",
    "\n",
    "Commentaire:\n",
    "Le meilleur moyen de trouver les hyperparamètres optimaux est la validation croisée. Ici est représenté le score moyen testé sur tous les folds pour chaque k. On remarque que le meilleur k est égale à 1 car le score moyen est le plus élevé pour k = 1.\n",
    "\n",
    "Classe LDA\n",
    "Question 15 : Formule des probabilité a posteriori\n",
    "Demonstration\n",
    "1. Formule de Bayes nous dit que:\n",
    "$$ Pr{ ( Y = 1 | X = x ) } =\\frac{P{ (X | Y = 1 ) } Pr { (Y = 1 )}}{P{ (X)}} $$\n",
    "et$$ Pr{ ( Y = -1 | X = x ) } =\\frac{P{ (X | Y = -1 ) } Pr { (Y = -1 )}}{P{ (X)}} $$\n",
    "\n",
    "2. Formule des probabilités totales nous dit que :\n",
    "$$ P{ (X)} = P{ (X | Y = 1 ) } Pr { (Y = 1 )} + P{ (X | Y = -1 ) } Pr { (Y = -1 )} $$\n",
    "d'où le résultat:\n",
    "\n",
    "$$ Pr{ ( Y = 1 | X = x ) } = \\frac { pi_{+}f_{+} }{ pi_{+}f_{+} + pi_{-}f_{-}} $$\n",
    "Résultat\n",
    "$$ Pr{ ( Y = -1 | X = x ) } =\\frac { pi_{-}f_{-} }{ pi_{+}f_{+} + pi_{-}f_{-}} $$\n",
    "Question 16 : Log-ration des 2 classes\n",
    "Pour implémenter la classification LDA, il nous faut faire l'hypothèse que les distributions des différentes classes sont gaussiennes. Ainsi dans notre cas où nous avons 2 classes, les densités sont égales à :\n",
    "\n",
    "$$  f_{+} = \\frac{1}{ (2 \\pi) ^ {\\frac{p}{2}} \\sqrt{\\det(\\Sigma)} } \n",
    "\\exp {   (-\\frac{1}{2} ({(x- \\mu_{+})^T}\n",
    " \\Sigma^{-1} ((x- \\mu_{+})) \n",
    "}  \n",
    "$$\n",
    "et\n",
    "\n",
    "$$  f_{-} = \\frac{1}{ (2 \\pi) ^ {\\frac{p}{2}} \\sqrt{\\det(\\Sigma)} } \n",
    "\\exp {   (-\\frac{1}{2} ({(x- \\mu_{-})^T}\n",
    " \\Sigma^{-1} ((x- \\mu_{-})) \n",
    "}  \n",
    "$$\n",
    "d'où la probabilité a postériori s'écrit :\n",
    "\n",
    "$$ Pr{ ( Y = 1 | X = x ) } = \\frac { \\frac{m}{n}  f_{+} }{\\frac{m}{n} f_{+} + (1-\\frac{m}{n}) f_{-}}  $$\n",
    "et\n",
    "\n",
    "$$ \\frac{Pr{ ( Y = 1 | X = x ) }}{Pr{ ( Y =- 1 | X = x ) }}  =   \\frac { \\frac{m}{n}  f_{+} }{ (1-\\frac{m}{n})  f_{-} }$$\n",
    "En prenant le log on obtient :\n",
    "\n",
    "$$ \\log(\\frac{Pr{ ( Y = 1 | X = x ) }}{Pr{ ( Y =- 1 | X = x ) }}) =  \\log ( \\frac{m}{n}  f_{+}    ) - \n",
    "\\log ( (1-\\frac{m}{n}) f_{-}  ))\n",
    "=  \\log ( \\frac{m}{n}) + \\log (f_{+}  ) - \\log (f_{-} ) - \\log ( (1-\\frac{m}{n})\n",
    "$$\n",
    "$$\\log {f_{+} } =  (-\\frac{1}{2} ({(x- \\mu_{+})^T}\n",
    " \\Sigma^{-1} ((x- \\mu_{+})  (-( \\log(2 \\pi ) ( (\\frac{p}{2}))  +  \\frac{1}{2}  \\log {\\det {\\Sigma} } ) $$$$\\log {f_{-} } =  (-\\frac{1}{2} ({(x- \\mu_{-})^T}\n",
    " \\Sigma^{-1}((x- \\mu_{-})  (-( \\log(2 \\pi ) (\\frac{p}{2}))  + \\frac{1}{2}  \\log {\\det {\\Sigma} } ) $$\n",
    "$$ \\log(\\frac{Pr{ ( Y = 1 | X = x ) }}{Pr{ ( Y =- 1 | X = x ) }}) = \\log(m/n)+\\log(f_{+}) -\\log(1-\\frac{m}{n})-\\log(f_{-})$$\n",
    "\n",
    "$$ \\log(\\frac{Pr{ ( Y = 1 | X = x ) }}{Pr{ ( Y =- 1 | X = x ) }}) = x^{T} \\Sigma^{-1}(\\mu_{+}-\\mu_{-}) -\\frac{1}{2} \\mu^{T}_{+}\\Sigma^{-1}\\mu_{+}    \n",
    "+ \\frac{1}{2} \\mu^{T}_{-}\\Sigma^{-1}\\mu_{-} - \\log(1-\\frac{m}{n}) + \\log(\\frac{m}{n})$$\n",
    "Question 17 : Choix du classifieur\n",
    "Si la classe est égale à 1 alors cela signifie que la probabilité a posteriori de la classe égale à 1 est supèrieure à l'autre classe d'où$$\\log(\\frac{Pr{ ( Y = 1 | X = x ) }}{Pr{ ( Y =- 1 | X = x ) }}) &gt; 1$$alors d'après Question 16 $$ x^{T} \\Sigma^{-1}(\\mu{+}-\\mu{-}) -\\frac{1}{2} \\mu^{T}{+}\\Sigma^{-1}\\mu{+}\n",
    "\n",
    "\\frac{1}{2} \\mu^{T}{-}\\Sigma^{-1}\\mu{-} - \\log(1-\\frac{m}{n}) + \\log(\\frac{m}{n}) > 0$$\n",
    "d'où\n",
    "$$x^{T} \\Sigma^{-1}(\\mu{+}-\\mu{-}) >\\frac{1}{2} \\mu^{T}{+}\\Sigma^{-1}\\mu{+}\n",
    "\\frac{1}{2} \\mu^{T}{-}\\Sigma^{-1}\\mu{-} + \\log(1-\\frac{m}{n}) - \\log(\\frac{m}{n}) $$ et même raisonnement pour la classe égale à -1.\n",
    "Question 18 : Implémentation artisanale du classifieur LDA\n",
    "In [25]:\n",
    "class LDAClassifier(BaseEstimator, ClassifierMixin):\n",
    "#\"\"\" Homemade kNN classifier class \"\"\"\n",
    "    #Define the training set\n",
    "    #def __init__(self):\n",
    "        #assert (k <= len(x)), \"k cannot be greater than training_set length\"\n",
    "    # Complete the mehtod\n",
    "    def fit(self, X, y):       \n",
    "        Xp = X[y == 1, :]\n",
    "        Xn = X[y == -1, :]\n",
    "        Xp_x = Xp[:,0]\n",
    "        Xp_y = Xp[:,1]\n",
    "        n = len(X)\n",
    "        m = len(Xp)\n",
    "        meanp_x = np.mean(Xp_x)\n",
    "        meanp_y = np.mean(Xp_y)\n",
    "        meanp = np.array([meanp_x,meanp_y])\n",
    "        Cov_p = np.cov(np.transpose(Xp))\n",
    "        Xn_x = Xn[:,0]\n",
    "        Xn_y = Xn[:,1]\n",
    "\n",
    "        meann_x = np.mean(Xn_x)\n",
    "        meann_y = np.mean(Xn_y)\n",
    "        meann = np.array([meann_x,meann_y])\n",
    "        Cov_n = np.cov(np.transpose(Xn))\n",
    "        \n",
    "        Cov_tot = (1/(n-2))*( (m-1)* Cov_p + (n-m-1)* Cov_n)\n",
    "        inv_Cov_tot = np.linalg.inv(Cov_tot)\n",
    "        \n",
    "        a1 = np.dot(np.transpose(meanp),inv_Cov_tot)\n",
    "        a2 = np.dot(np.transpose(meann),inv_Cov_tot)\n",
    "        \n",
    "        self.alpha = 0.5*(np.dot(a1,meanp)  - np.dot(a2,meann)) + np.log(1- m/n) - np.log(m/n)\n",
    "        self.beta =  np.dot(inv_Cov_tot,meanp-meann)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_=[]\n",
    "        for i in range(len(X)):\n",
    "            Xpred = X[i]\n",
    "            b = np.dot(np.transpose(Xpred), self.beta)\n",
    "            if (b>self.alpha):\n",
    "                Ypred = 1\n",
    "            else:\n",
    "                Ypred = -1\n",
    "            y_.append(Ypred)\n",
    "        return np.array(y_)\n",
    "Question 19 : Comparaison avec Sklearn library\n",
    "Cas où les échantillons ont une classe plus nombreuse qu'une autre.\n",
    "Cas où les données d'entrainement sont plus petites que les données test (ce qui est demandé dans l'énoncé et\n",
    "que je trouve bizarre car en principe on devrait faire en sorte d'avoir plus de données pour entrainer le modèle\n",
    "que pour le tester...)\n",
    "In [38]:\n",
    "plt.figure(figsize=(15,8))\n",
    "X1,y1=rand_bi_gauss(n1=100, n2=20, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.9, 0.9], sigmas2=[0.9, 0.9])\n",
    "#X2,y2=rand_bi_gauss(n1=100, n2=20, mu1=[1, 1], mu2=[-1, -1], sigmas1=[1, 1], sigmas2=[1, 1])\n",
    "test= LDAClassifier()\n",
    "test.fit(X1[:50],y1[:50])\n",
    "test.predict(X1[50:])\n",
    "def f1(x):\n",
    "    return test.predict(x)\n",
    "plt.subplot(121)\n",
    "plt.title(\"Librairie artisanale\")\n",
    "frontiere_new(f1,X1[:50],y1[:50])\n",
    "erreur = 1-test.score(X1[50:],y1[50:])\n",
    "print(\"Le taux d'erreur sur les données test avec la librairie artisanale est : \" + str(round(erreur,3)))\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "testsklearn=LinearDiscriminantAnalysis()\n",
    "testsklearn.fit(X1[:50],y1[:50])\n",
    "testsklearn.predict(X1[50:])\n",
    "def f2(x):\n",
    "    return testsklearn.predict(x)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Librarie Sklearn\")\n",
    "frontiere_new(f2,X1[:50],y1[:50])\n",
    "erreur = 1-testsklearn.score(X1[50:],y1[50:])\n",
    "print(\"Le taux d'erreur sur les données test avec la librairie sklearn est : \" + str(round(erreur,3)))\n",
    "Le taux d'erreur sur les données test avec la librairie artisanale est : 0.029\n",
    "Le taux d'erreur sur les données test avec la librairie sklearn est : 0.029\n",
    "\n",
    "Cas où les données d'entrainement sont plus grandes que les données test (ce que je trouve plus pertinent)\n",
    "In [27]:\n",
    "plt.figure(figsize=(15,8))\n",
    "X1,y1=rand_bi_gauss(n1=200, n2=40, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.9, 0.9], sigmas2=[0.9, 0.9])\n",
    "\n",
    "test= LDAClassifier()\n",
    "test.fit(X1[50:],y1[50:])\n",
    "test.predict(X1[:50])\n",
    "def f1(x):\n",
    "    return test.predict(x)\n",
    "plt.subplot(121)\n",
    "plt.title(\"Handmade library\")\n",
    "frontiere_new(f1,X1[:50],y1[:50])\n",
    "erreur = 1-test.score(X1[:50],y1[:50])\n",
    "\n",
    "print(\"Le taux d'erreur sur les données test avec la librairie artisanale est : \" + str(round(erreur,3)))\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "testsklearn=LinearDiscriminantAnalysis()\n",
    "testsklearn.fit(X1[50:],y1[50:])\n",
    "testsklearn.predict(X1[:50])\n",
    "def f2(x):\n",
    "    return testsklearn.predict(x)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Sklearn library\")\n",
    "frontiere_new(f2,X1[:50],y1[:50])\n",
    "erreur = 1-testsklearn.score(X1[:50],y1[:50])\n",
    "print(\"Le taux d'erreur sur les données test avec la librairie sklearn est : \" + str(round(erreur,3)))\n",
    "Le taux d'erreur sur les données test avec la librairie artisanale est : 0.04\n",
    "Le taux d'erreur sur les données test avec la librairie sklearn est : 0.04\n",
    "\n",
    "Cas où le nombre de classe est le même\n",
    "In [28]:\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import f1_score\n",
    "plt.figure(figsize=(15,8))\n",
    "X1,y1=rand_bi_gauss(n1=200, n2=200, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.1, 0.1], sigmas2=[0.1, 0.1])\n",
    "#X2,y2=rand_bi_gauss(n1=1000, n2=20, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.1, 0.1], sigmas2=[0.1, 0.1])\n",
    "test= LDAClassifier()\n",
    "test.fit(X1[:150],y1[:150])\n",
    "test.predict(X1[150:])\n",
    "def f1(x):\n",
    "    return test.predict(x)\n",
    "plt.subplot(121)\n",
    "plt.title(\"Handmade library\")\n",
    "frontiere_new(f2,X1[150:],y1[150:])\n",
    "erreur = 1-test.score(X1[150:],y1[150:])\n",
    "\n",
    "print(\"Le taux d'erreur sur les données test avec la librairie artisanale est : \" + str(round(erreur,3)))\n",
    "testsklearn=LinearDiscriminantAnalysis()\n",
    "testsklearn.fit(X1,y1)\n",
    "testsklearn.predict(X2)\n",
    "\n",
    "erreur = 1-testsklearn.score(X1[150:],y1[150:])\n",
    "print(\"Le taux d'erreur sur les données test avec la librairie sklearn est : \" + str(round(erreur,3)))\n",
    "def f2(x):\n",
    "    return testsklearn.predict(x)\n",
    "plt.subplot(122)\n",
    "plt.title(\"Sklearn library\")\n",
    "frontiere_new(f2,X1[150:],y1[150:])\n",
    "Le taux d'erreur sur les données test avec la librairie artisanale est : 0.0\n",
    "Le taux d'erreur sur les données test avec la librairie sklearn est : 0.0\n",
    "\n",
    "Question 20 : Validité de la classe LDA Classifier sur les jeux de données 1 et 3\n",
    "Jeux de données 1 : Classification LDA de données provenant d'un mélange de distribution gaussienne\n",
    "In [29]:\n",
    "X1, y1 = rand_bi_gauss(n1 = 30, n2 = 30, mu1 = [1, 1], mu2 = [-1, -1],\n",
    "                      sigmas1 = [0.9, 0.9], sigmas2 = [0.9, 0.9])\n",
    "\n",
    "test = LDAClassifier()\n",
    "test.fit(X1[::2],y1[::2])\n",
    "plt.figure(figsize=(15,8))\n",
    "def f1(x):\n",
    "    return test.predict(x)\n",
    "plt.subplot(121)\n",
    "plt.title('Handmade library')\n",
    "frontiere_new(f1,X1[1::2],y1[1::2])\n",
    "erreur=1-test.score(X1[1::2],y1[1::2])# round(1-metrics.accuracy_score(expected, predicted),3)\n",
    "print(\"Le taux d'erreur sur les données test avec la librairie artisanale est : \" + str(round(erreur,3)))\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "testsklearn=LinearDiscriminantAnalysis()\n",
    "testsklearn.fit(X1[::2],y1[::2])\n",
    "\n",
    "def f2(x):\n",
    "    return testsklearn.predict(x)\n",
    "plt.subplot(122)\n",
    "plt.title('Sklearn library')\n",
    "frontiere_new(f2,X1[1::2],y1[1::2])\n",
    "\n",
    "erreur=1-testsklearn.score(X1[1::2],y1[1::2])# round(1-metrics.accuracy_score(expected, predicted),3)\n",
    "print(\"Le taux d'erreur sur les données test avec la librairie sklearn est : \" + str(round(erreur,3)))\n",
    "Le taux d'erreur sur les données test avec la librairie artisanale est : 0.033\n",
    "Le taux d'erreur sur les données test avec la librairie sklearn est : 0.033\n",
    "\n",
    "Jeux de données 3 : Classification LDA de données provenant d'un mélange de distribution non gaussienne\n",
    "In [30]:\n",
    "n1 = 150\n",
    "n2 = 150\n",
    "sigmas1 = 0.1\n",
    "sigmas2 = 0.1\n",
    "X3, y3 = rand_clown(n1, n2, sigmas1, sigmas2)\n",
    "In [31]:\n",
    "n1 = 150\n",
    "n2 = 150\n",
    "sigmas1 = 0.1\n",
    "sigmas2 = 0.1\n",
    "X3, y3 = rand_clown(n1, n2, sigmas1, sigmas2)\n",
    "X4, y4 = rand_clown(n1, n2, sigmas1, sigmas2)\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "\n",
    "test= LDAClassifier()\n",
    "test.fit(X3[::2],y3[::2])\n",
    "def f1(x):\n",
    "    return test.predict(x)\n",
    "plt.subplot(121)\n",
    "plt.title('Handmade library')\n",
    "frontiere_new(f1,X3[1::2],y3[1::2])\n",
    "erreur=1-test.score(X3[1::2],y3[1::2])\n",
    "print(\"Le taux d'erreur sur les données test est : \" + str(round(erreur,3)))\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "testsklearn=LinearDiscriminantAnalysis()\n",
    "testsklearn.fit(X3[::2],y3[::2])\n",
    "\n",
    "def f2(x):\n",
    "    return testsklearn.predict(x)\n",
    "plt.subplot(122)\n",
    "plt.title('Sklearn library')\n",
    "frontiere_new(f2,X3[1::2],y3[1::2])\n",
    "erreur=1-testsklearn.score(X3[1::2],y3[1::2])\n",
    "print(\"Le taux d'erreur sur les données test est : \" + str(round(erreur,3)))\n",
    "Le taux d'erreur sur les données test est : 0.107\n",
    "Le taux d'erreur sur les données test est : 0.107\n",
    "\n",
    "Observation :\n",
    "Cas du jeu de données 1 où les données sont bien classifiées. Ce jeu de données suit un mélange de distributions gaussiennes, ainsi le classifieur LDA est bien adapté à ce cas. En effet il permet de bien séparer ce jeu de donnée car les hypothèses de gaussianité sur lesquels reposent ce clasiffieur sont bien établies.\n",
    "Dans le cas du jeu de données 3 on obtient une mauvaise classification. En effet les données ne suivent aucunement des distributions gaussiennes, ainsi on ne peut pas apliquer la LDA. Cependant dans ce cas de jeux de données, le classifieur knn_voisins est approprié.\n",
    "Régression logistique\n",
    "Pour le cas où les données sont linéairement séparables, la régression logistique devrait bien fonctionner sans nécessité d'avoir une hypothèse de distribution gaussienne sur les données.\n",
    "\n",
    "Question 21 : Classification par régression logistique\n",
    "Cas où les classes sont déséquilibrées\n",
    "In [32]:\n",
    "X1,y1=rand_bi_gauss(n1=500, n2=100, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.9, 0.9], sigmas2=[0.9, 0.9])\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X1[::2],y1[::2])\n",
    "lr.predict(X1[1::2])\n",
    "\n",
    "lda=LinearDiscriminantAnalysis()\n",
    "lda.fit(X1[::2],y1[::2])\n",
    "lda.predict(X1[1::2])\n",
    "\n",
    "\n",
    "\n",
    "#Taux d'erreur\n",
    "\n",
    "errlda=1-lda.score(X1[1::2],y1[1::2])\n",
    "errlr=1-lr.score(X1[1::2],y1[1::2])\n",
    "\n",
    "print(f\"Le taux d'erreur avec la LDA est {round(errlda,2)}\")\n",
    "print(f\"Le taux d'erreur avec la regression logistique est {round(errlr,2)}\")\n",
    "Le taux d'erreur avec la LDA est 0.05\n",
    "Le taux d'erreur avec la regression logistique est 0.05\n",
    "Cas où les classes sont égales\n",
    "In [33]:\n",
    "X1,y1=rand_bi_gauss(n1=500, n2=500, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.9, 0.9], sigmas2=[0.9, 0.9])\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X1[::2],y1[::2])\n",
    "lr.predict(X1[1::2])\n",
    "\n",
    "lda=LinearDiscriminantAnalysis()\n",
    "lda.fit(X1[::2],y1[::2])\n",
    "lda.predict(X1[1::2])\n",
    "\n",
    "\n",
    "\n",
    "#Taux d'erreur\n",
    "\n",
    "errlda=1-lda.score(X1[1::2],y1[1::2])\n",
    "errlr=1-lr.score(X1[1::2],y1[1::2])\n",
    "\n",
    "print(f\"Le taux d'erreur avec la LDA est {round(errlda,2)}\")\n",
    "print(f\"Le taux d'erreur avec la régression logistique est {round(errlr,2)}\")\n",
    "Le taux d'erreur avec la LDA est 0.04\n",
    "Le taux d'erreur avec la régression logistique est 0.04\n",
    "Question 22 : Interprétation des variables coef et intercept\n",
    "On observe que la régression logistique est bien une méthode de séparation des données linéaires\n",
    "\n",
    "Les coefficient et intercept dans la méthode de la régression logistique caractérise l'hyperplan séparant linéairement les différentes classes. Cet hyperplan a pour équation$$y = w_{0} + w_{1}x^{T}$$où l'intercept est$$w_{0}$$et le coefficient est$$w_{1}$$\n",
    "\n",
    "In [34]:\n",
    "print(f\"Dans notre cas le vecteur coefficient est \" +str(lr.coef_) + \" et l'intercept est \" + str(lr.intercept_))\n",
    "Dans notre cas le vecteur coefficient est [[2.70840576 2.04748771]] et l'intercept est [-0.18490694]\n",
    "Question 23 : Frontière de décision pour les 2 classifieurs linéaires\n",
    "In [37]:\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X1[::2],y1[::2])\n",
    "def f1(x):\n",
    "    return lr.predict(x)\n",
    "\n",
    "\n",
    "lda=LinearDiscriminantAnalysis()\n",
    "lda.fit(X1[::2],y1[::2])\n",
    "\n",
    "def f2(x):\n",
    "    return lda.predict(x)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121)\n",
    "plt.title(\"Logistic regression\")\n",
    "\n",
    "frontiere_new(f2,X1[1::2],y1[1::2])\n",
    "plt.subplot(122)\n",
    "plt.title(\"Linear Discrimant analysis (LDA)\")\n",
    "frontiere_new(f1,X1[1::2],y1[1::2])\n",
    "\n",
    "Observation\n",
    "La frontière de décisions dans le cas des classifieurs linéaire est la même.\n",
    "\n",
    "Question 24 : Régression logistique à la base DIGITS\n",
    "In [36]:\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 4 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset.\n",
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)\n",
    "\n",
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create a classifier: Logistic Regression\n",
    "\n",
    "classifier=LogisticRegression()\n",
    "# We learn the digits on the first half of the digits\n",
    "\n",
    "classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n",
    "# Now predict the value of the digit on the second half:\n",
    "\n",
    "expected= digits.target[n_samples // 2:]\n",
    "predicted = classifier.predict(data[n_samples // 2:])\n",
    "\n",
    "\n",
    "\n",
    "#plt.show()----------\n",
    "print('--------------------------------------------------------------------------')\n",
    "print(f\"                    Le taux d'erreur est {round(1-metrics.r2_score(expected, predicted),3)}\")\n",
    "print('--------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
